# @package _global_
# Adversarial Training HPO with pretrained model (PGD-AT with curriculum)
#
# Skips classification pretraining — loads a pretrained model instead.
# This avoids re-running pretraining for every HPO trial.
#
# IMPORTANT: dataset.split_seed must match the split_seed used during
# pretraining (moons_4k default: 11) to ensure identical train/valid/test splits.
#
# Usage:
#   python -m experiments.adversarial --multirun +experiments=adversarial/fourier_d30D18/seed_sweeps/[dataset_name]


experiment: seed_sweep

defaults:
  - override /born: fourier/d30D18
  - override /trainer/classification: null  # Skip pretraining — use model_path
  - override /trainer/adversarial: pgd_at
  - override /trainer/ganstyle: null
  - override /trainer/generative: null
  - override /tracking: online

# Pretrained model path (required — Hydra will error if not provided)
model_path: /mathqi/mnissen/adversarial_training_mps/outputs/cls_seed_sweep_circles_4k_10Feb26/2/models/circles_4k_pre_mps_bd18_fourier30_500adamlr0.005669446758159133wd

# Ensure split matches pretraining
dataset:
  name: "circles_4k"
  gen_dow_kwargs:
    name: "circles_4k"
    size: 4000
    seed: 25
    noise: 0.05
    circ_factor: 0.3
  split: [0.5, 0.25, 0.25] # train, val, test
  split_seed: 11
  overwrite: True

# W&B settings
tracking:
  evasion:
    method: "PGD"
    norm: "inf"
    criterion:
      name: "negative log-likelihood"
      kwargs:
        eps: 1e-8
    strengths: [0.15]
    num_steps: 20
    step_size: null
    random_start: true

# Adversarial training settings
trainer:
  adversarial:
    max_epoch: 500
    batch_size: 256
    method: "pgd_at"
    stop_crit: "rob"
    patience: 200
    watch_freq: 2000
    # Evaluate clean and robust accuracy every epoch
    # FID, genloss, viz every 50 epochs (expensive)
    metrics: {"clsloss": 1, "acc": 1, "rob": 1, "fid": 50, "genloss": 50, "viz": 50}

    # Attack settings (strong PGD attack)
    evasion:
      method: "PGD"
      norm: "inf"
      criterion:
        name: "negative log-likelihood"
        kwargs:
          eps: 1e-8
      strengths: [0.15]
      num_steps: 10
      step_size: null
      random_start: true

    # Curriculum: ramp epsilon from 0.01 to 0.15 over first 300 epochs
    curriculum: true
    curriculum_start: 0.01
    curriculum_end_epoch: 300

    save: True
    auto_stack: true
    auto_unbind: false

    # Optimizer pareto optimal:
    optimizer:
      kwargs:
        lr: 0.000313
        weight_decay: 3.310170e-09
      clean_weight: 0.224138

hydra:
  sweeper:
    params:
      tracking.seed: choice(123, 245, 124, 356, 23)