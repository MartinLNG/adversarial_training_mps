# @package _global_
# Adversarial Training HPO with pretrained model (PGD-AT with curriculum)
#
# Skips classification pretraining — loads a pretrained model instead.
# This avoids re-running pretraining for every HPO trial.
#
# IMPORTANT: dataset.split_seed must match the split_seed used during
# pretraining (moons_4k default: 11) to ensure identical train/valid/test splits.
#
# Optimizes: lr, weight_decay, clean_weight
# Uses strong PGD attack with curriculum training (epsilon: 0.01 -> 0.15)
#
# Usage:
#   # Run for spirals dataset (100 trials overnight)
#   python -m experiments.adversarial --multirun +experiments=adversarial/fourier_d30D18/hpo/spirals \
#       model_path=/path/to/pretrained/model.pt
#
#   # Quick test (fewer trials)
#   python -m experiments.adversarial --multirun +experiments=adversarial/fourier_d30D18/hpo/spirals \
#       model_path=/path/to/pretrained/model.pt \
#       hydra.sweeper.n_trials=5 trainer.adversarial.max_epoch=50

experiment: hpo

defaults:
  - override /born: d30D18
  - override /dataset: 2Dtoy/spirals_4k
  - override /trainer/classification: null  # Skip pretraining — use model_path
  - override /trainer/adversarial: pgd_at
  - override /trainer/ganstyle: null
  - override /trainer/generative: null
  - override /tracking: online
  - override /hydra/sweeper: optuna

# Pretrained model path (required — Hydra will error if not provided)
model_path: /home/ubuntu/adversarial_training_mps/outputs/models/spirals

# Ensure split matches pretraining
dataset:
  split_seed: 11
  gen_dow_kwargs:
    seed: 25

# W&B settings
tracking:
  seed: 2 # needs to be adjusted to match pretraining
  evasion:
    method: "PGD"
    norm: "inf"
    criterion:
      name: "negative log-likelihood"
      kwargs:
        eps: 1e-8
    strengths: [0.15]
    num_steps: 20
    step_size: null
    random_start: true

# Adversarial training settings
trainer:
  adversarial:
    max_epoch: 500
    batch_size: 256
    method: "pgd_at"
    stop_crit: "rob"
    patience: 200
    watch_freq: 2000
    # Evaluate clean and robust accuracy every epoch
    # FID, genloss, viz every 50 epochs (expensive)
    metrics: {"clsloss": 1, "acc": 1, "rob": 1, "fid": 50, "genloss": 50, "viz": 50}

    # Attack settings (strong PGD attack)
    evasion:
      method: "PGD"
      norm: "inf"
      criterion:
        name: "negative log-likelihood"
        kwargs:
          eps: 1e-8
      strengths: [0.15]
      num_steps: 20
      step_size: null
      random_start: true

    # Curriculum: ramp epsilon from 0.01 to 0.15 over first 300 epochs
    curriculum: true
    curriculum_start: 0.01
    curriculum_end_epoch: 300

    save: false
    auto_stack: true
    auto_unbind: false

# Optuna HPO configuration
hydra:
  sweeper:
    # Storage set to null = in-memory (or override on command line)
    storage: null
    study_name: "${experiment}_${dataset.name}"

    sampler:
      _target_: optuna.samplers.TPESampler
      seed: 42
      n_startup_trials: 15

    direction: minimize  # Minimizing -rob_acc (negated in entry point)
    n_trials: 100
    n_jobs: 1

    # Search space (no tracking.seed — fixed since pretraining is not re-run)
    params:
      # Learning rate (log scale, inspired by classification HPO range)
      trainer.adversarial.optimizer.kwargs.lr: tag(log, interval(1e-5, 1e-2))
      # Weight decay (log scale)
      trainer.adversarial.optimizer.kwargs.weight_decay: tag(log, interval(1e-9, 1e-3))
      # Clean weight: 0 = pure adversarial, 0.5 = half clean/half adversarial
      trainer.adversarial.clean_weight: interval(0.0, 0.5)
