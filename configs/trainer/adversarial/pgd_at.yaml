# PGD Adversarial Training (Madry et al.)
# Trains on adversarial examples: Loss = L(x_adv, y)
max_epoch: 100
batch_size: 64
method: "pgd_at"

optimizer:
  name: "adam"
  kwargs:
    lr: 1e-4
    weight_decay: 0.0

criterion:
  name: "negative log-likelihood"
  kwargs:
    eps: 1e-8

evasion:
  method: "PGD"
  norm: "inf"
  criterion:
    name: "negative log-likelihood"
    kwargs:
      eps: 1e-8
  strengths: [0.1]
  num_steps: 10
  step_size: null  # defaults to 2.5 * strength / num_steps
  random_start: true

stop_crit: "rob"
patience: 30
watch_freq: 500

metrics: {"loss": 1, "acc": 1, "rob": 5}

trades_beta: 6.0  # ignored for pgd_at
clean_weight: 0.0  # pure adversarial (set > 0 to mix with clean examples)
curriculum: false
curriculum_start: 0.0
curriculum_end_epoch: null

save: true
auto_stack: true
auto_unbind: false
